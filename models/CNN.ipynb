{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CNN that can identify family members once it has been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.relpath(\"../\"))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting dimension prior to flatten layer is 10\n"
     ]
    }
   ],
   "source": [
    "# Custom CNN model to identify images, expects data to have dimensions of [batch_size, color channel, pic_height, pic_width]\n",
    "KERNEL_SIZE = 3\n",
    "POOL_KERNEL = 2\n",
    "STRIDE = 1\n",
    "PADDING = 2\n",
    "\n",
    "layer_dim_calc(28, 2, 2, KERNEL_SIZE, POOL_KERNEL, STRIDE, PADDING)\n",
    "\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, input, hidden_units, output):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=KERNEL_SIZE,\n",
    "                      padding=PADDING),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=KERNEL_SIZE,\n",
    "                      padding=PADDING),            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=POOL_KERNEL)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=KERNEL_SIZE,\n",
    "                      padding=PADDING),            \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=KERNEL_SIZE,\n",
    "                      padding=PADDING),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=POOL_KERNEL)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_units* pow(layer_dim_calc(28,\n",
    "                                                       2, \n",
    "                                                       2,\n",
    "                                                       KERNEL_SIZE, \n",
    "                                                       POOL_KERNEL,\n",
    "                                                       STRIDE,\n",
    "                                                       PADDING\n",
    "                                                        ), 2), \n",
    "                      output) # need to be aware of our photo dims after being filtered by conv2d and maxpool2d layers\n",
    "         )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.flatten(x)\n",
    "    \n",
    "    \n",
    "# Custom dataset class that inherits from torch.utils.data.Dataset\n",
    "class FamilyData(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.image_paths = []\n",
    "\n",
    "        # Data format (image, image_idx)\n",
    "        for idx, class_name in enumerate(self.classes):\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for img_name in os.listdir(class_folder):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    if img_path.endswith('.jpg') or img_path.endswith('.png'):\n",
    "                        self.image_paths.append((img_path, class_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB') # Image dim = [3, 224, 224] \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            image = image.permute(1, 2, 0) # rearrange dimensions to be compatible with plt.imgshow()\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, dataset_loader: torch.utils.data.DataLoader, lr=0.1):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, label) in enumerate(dataset_loader):\n",
    "        data, label = next(iter(dataset_loader))\n",
    "        \n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1) % int(len(dataset_loader)/5) == 0:\n",
    "            print(f\"Ran {batch_idx+1} loops | loss: {loss.item()}\")\n",
    "\n",
    "def test_model(model: nn.Module, dataset_loader: torch.utils.data.DataLoader):\n",
    "    test_accur, test_loss = 0, 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for data, labels in dataset_loader:\n",
    "            logits = model(data)\n",
    "            test_loss += loss_fn(logits, labels)\n",
    "            test_accur += (logits.argmax(1) == labels.squeeze()).float().mean() *100\n",
    "\n",
    "        test_loss /= len(dataset_loader)\n",
    "        test_accur /= len(dataset_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_accur:.2f}%\\n\")\n",
    "\n",
    "            # if accuracy * 100 > 98:\n",
    "            #     FOLDER_NAME = \"../state_dict\"\n",
    "            #     MODEL_NAME = \"Four_Classes_Multiclassfication_Model.pt\"\n",
    "            #     torch.save(model.state_dict(), f\"{FOLDER_NAME}/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image datasets\n",
    "root_folder = Path(\"../data/Family_PP\")\n",
    "\n",
    "# Set the transformation of the image data to be 224 x 224 resolution and tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "dataset = FamilyData(root_folder, transform=transform)\n",
    "\n",
    "# Check the images in dataset\n",
    "# for idx, image_data in enumerate(dataset):\n",
    "#     image, label = image_data \n",
    "#     plt.figure()\n",
    "#     plt.title(f\"{label}\")\n",
    "#     plt.axis(False)\n",
    "#     plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(root='.././data', train=True, download=False, transform=ToTensor())\n",
    "test_Data = datasets.FashionMNIST(root='.././data', train=False, download=False, transform=ToTensor())\n",
    "\n",
    "# fig = plt.figure(figsize=(9, 9))\n",
    "# rows, col = 4, 4\n",
    "# for i in range(1, (rows*col + 1)):\n",
    "#     random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "#     img, label = train_data[random_idx]\n",
    "#     fig.add_subplot(rows, col, i)\n",
    "#     plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "#     plt.title(class_names[label])\n",
    "#     plt.axis(False)\n",
    "\n",
    "len(train_data)\n",
    "len(test_Data)\n",
    "\n",
    "length = DataLoader(train_data, batch_size=32)\n",
    "len(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting dimension prior to flatten layer is 10\n",
      "Ran 375 loops | loss: 0.40972375869750977\n",
      "Ran 750 loops | loss: 0.5317894816398621\n",
      "Ran 1125 loops | loss: 0.44580069184303284\n",
      "Ran 1500 loops | loss: 0.6202183365821838\n",
      "Ran 1875 loops | loss: 0.2446848452091217\n",
      "Test loss: 0.40258 | Test accuracy: 85.15%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.FashionMNIST(root='.././data', train=True, download=False, transform=ToTensor())\n",
    "test_Data = datasets.FashionMNIST(root='.././data', train=False, download=False, transform=ToTensor())\n",
    "\n",
    "dataset_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_Data, batch_size=32, shuffle=True)\n",
    "\n",
    "COLOR_CHANNELS = 1\n",
    "CLASSES = len(train_data.classes)\n",
    "LOOPS = 1000\n",
    "\n",
    "model = CNN_Model(input=COLOR_CHANNELS, hidden_units=10, output=CLASSES)\n",
    "train_model(model, dataset_loader, lr=0.1)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "data, label = next(iter(dataset_loader))\n",
    "# print(torch.softmax(model(data[0].unsqueeze(0)).squeeze(), 0).argmax(0))\n",
    "# print(label[0])\n",
    "# print(model(data).shape)\n",
    "\n",
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 10, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "images = torch.randn(size=(32, 3, 28, 28))\n",
    "test_image = images[0].unsqueeze(0)\n",
    "print(test_image.shape)\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels=3,\n",
    "                   out_channels=10,\n",
    "                   kernel_size=3,\n",
    "                   stride=1,\n",
    "                   padding=0)\n",
    "\n",
    "print(conv2d(test_image).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Resulting dimension of 8.5 prior to flatten layer is not an integer. This may impact model performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv2d_calc(input_dim: int, kernel, stride, padding):\n",
    "    output = 1 + (input_dim + (padding * 2) - kernel) / stride\n",
    "\n",
    "    return output\n",
    "\n",
    "# Calculate the resulting dim after filering through conv2d and maxpool2d\n",
    "# for epoch in range(0, 2, 1):\n",
    "#     for epoch in range(0, 2, 1):\n",
    "#         dim = conv2d_calc(dim, KERNEL_SIZE, STRIDE, PADDING)\n",
    "#     dim /= POOL_KERNEL\n",
    "\n",
    "def layer_dim_calc(input_dim, conv2d_count, maxpool_count, conv2d_kernel, maxpool_kernel, stride, padding):\n",
    "    \"\"\" Finds the resulting photo dimensions for a CNN model that uses conv2d and maxpool2d layers\n",
    "    \"\"\"\n",
    "    for epoch in range(0, maxpool_count, 1):\n",
    "        for epoch in range(0, conv2d_count, 1):\n",
    "            input_dim = conv2d_calc(input_dim, conv2d_kernel, stride, padding)\n",
    "        input_dim /= maxpool_kernel\n",
    "\n",
    "    if (input_dim != int(input_dim)):\n",
    "        print(f\"WARNING: Resulting dimension of {input_dim} prior to flatten layer is not an integer. This may impact model performance.\") \n",
    "    else:\n",
    "        print(f\"Resulting dimension prior to flatten layer is {int(input_dim)}\")\n",
    "    return int(input_dim)\n",
    "\n",
    "\n",
    "layer_dim_calc(28, 2, 2, KERNEL_SIZE, POOL_KERNEL, STRIDE, PADDING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
